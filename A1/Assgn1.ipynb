{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d1c889f-6e68-4dd8-b784-eaa7c43f0033",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "798f09f0-22f7-4feb-ba43-b49af05699e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P('quick') = 0.11\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def unigram_prob(word, corpus):\n",
    "    tokens = [word for sentence in corpus for word in sentence.split()]\n",
    "    unigram_counts = Counter(tokens)\n",
    "    N = len(tokens)\n",
    "    return unigram_counts[word] / N\n",
    "\n",
    "corpus = [\n",
    "    \"the quick brown fox jumps over the lazy dog\"\n",
    "]\n",
    "\n",
    "# Calculate P(\"the\")\n",
    "prob_the = unigram_prob(\"quick\", corpus)\n",
    "print(f\"P('quick') = {prob_the:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99336ede-490e-417d-90d0-81174c57ac8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<s>', 'I'), ('I', 'will'), ('will', 'be'), ('be', 'doing'), ('doing', 'my'), ('my', 'project'), ('project', 'soon'), ('soon', '</s>')]\n"
     ]
    }
   ],
   "source": [
    "def extract_bigrams(sentence):\n",
    "    words = sentence.split()\n",
    "    return list(zip(words[:-1], words[1:]))\n",
    "\n",
    "sentence = \"<s> I will be doing my project soon </s>\"\n",
    "bigrams = extract_bigrams(sentence)\n",
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a8105f2-cf6c-4e48-8e5e-89ba1ce6759e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P('project'|'the') = 0.12\n"
     ]
    }
   ],
   "source": [
    "def conditional_prob_bigram(w2, w1, bigrams, unigram_counts, laplace=True):\n",
    "    bigram = (w1, w2)\n",
    "    V = len(unigram_counts)  # Vocabulary size\n",
    "    count_bigram = bigrams.get(bigram, 0)\n",
    "    count_w1 = unigram_counts.get(w1, 0)\n",
    "    \n",
    "    if laplace:\n",
    "        return (count_bigram + 1) / (count_w1 + V)\n",
    "    else:\n",
    "        return count_bigram / count_w1 if count_w1 != 0 else 0\n",
    "\n",
    "all_tokens = [token for sentence in corpus for token in sentence.split()]\n",
    "\n",
    "unigram_counts = Counter(all_tokens)\n",
    "bigrams_corpus = [extract_bigrams(sentence) for sentence in corpus]\n",
    "bigram_counts = Counter(b for sentence_bigrams in bigrams_corpus for b in sentence_bigrams)\n",
    "\n",
    "# word = input() //Tried multiple things here, seems to be working as expected\n",
    "prob_cat_given_the = conditional_prob_bigram(\"only\", \"is\", bigram_counts, unigram_counts)\n",
    "print(f\"P('project'|'the') = {prob_cat_given_the:.2f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d366af4-4fc4-4c2d-a278-f5026c4c8ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next word after 'the': dog\n"
     ]
    }
   ],
   "source": [
    "def predict_next_word(w1, bigrams, unigram_counts):\n",
    "    candidates = [bigram[1] for bigram in bigrams if bigram[0] == w1]\n",
    "    if not candidates:\n",
    "        return None\n",
    "    return max(candidates, key=lambda x: bigrams.get((w1, x), 0))\n",
    "\n",
    "next_word = predict_next_word(\"lazy\", bigram_counts, unigram_counts)\n",
    "print(f\"Next word after 'the': {next_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "387192ab-ca5e-488b-ba96-797b8f341964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brown fox jumps over the quick brown fox jumps over\n"
     ]
    }
   ],
   "source": [
    "def predict_sentence(initial_phrase, bigram_counts, unigram_counts, max_length=10):\n",
    "    words = initial_phrase.split()\n",
    "    while len(words) < max_length:\n",
    "        last_word = words[-1]\n",
    "        next_word = predict_next_word(last_word, bigram_counts, unigram_counts)\n",
    "        if not next_word or next_word == \"</s>\":\n",
    "            break\n",
    "        words.append(next_word)\n",
    "    return \" \".join(words)\n",
    "\n",
    "initial_phrase = \"brown fox\"\n",
    "generated_sentence = predict_sentence(initial_phrase, bigram_counts, unigram_counts)\n",
    "print(generated_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2961775a-0517-47a7-ae8f-9c2427f6d046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_trigrams(sentence):\n",
    "    words = sentence.split()\n",
    "    return list(zip(words[:-2], words[1:-1], words[2:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63f5f761-960d-4e2a-8571-9d0b3297910e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.64.1)\n",
      "Collecting regex>=2021.8.3\n",
      "  Using cached regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.3)\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.9.1 regex-2024.11.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e70c281-e4e1-46cc-90ff-a713b1d3ec9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package webtext to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package webtext is already up-to-date!\n",
      "[nltk_data] Downloading package reuters to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"brown\")\n",
    "nltk.download(\"webtext\")\n",
    "nltk.download(\"reuters\")\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.corpus import brown, webtext, reuters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "571276f3-8d58-40cf-8ade-2d4e150e4131",
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_corpus = brown.sents()\n",
    "brown_corpus = [\" \".join(sentence) for sentence in brown_corpus]\n",
    "brown_corpus = [\"<s> \" + sentence + \" </s>\" for sentence in brown_corpus][:5000]\n",
    "webtext_corpus = webtext.sents()\n",
    "webtext_corpus = [\" \".join(sentence) for sentence in webtext_corpus]\n",
    "webtext_corpus = [\"<s> \" + sentence + \" </s>\" for sentence in webtext_corpus][:5000]\n",
    "reuters_corpus = reuters.sents()\n",
    "reuters_corpus = [\" \".join(sentence) for sentence in reuters_corpus]\n",
    "reuters_corpus = [\"<s> \" + sentence + \" </s>\" for sentence in reuters_corpus][:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "835e2866-08b3-491f-82a0-59ad97449931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question 1 Results:\n",
      "Brown Bigram Avg Perplexity: 2236.17\n",
      "Brown Trigram Avg Perplexity: 6643.72\n",
      "Expected Trigram perplexity to be lower, but it might be higher due to data sparsity\n",
      "\n",
      "Question 2 Results:\n",
      "Webtext Bigram Avg PPL on Reuters: 5978.34\n",
      "Webtext Trigram Avg PPL on Reuters: 11170.29\n",
      "\n",
      "Question 2 Results:\n",
      "Brown Bigram Avg PPL on Reuters: 6529.01\n",
      "Brown Trigram Avg PPL on Reuters: 13575.60\n",
      "\n",
      "Example Sentence Generation (Brown Trigrams):\n",
      "justice of Atlanta's recent primary election produced `` no evidence '' that any irregularities took place .\n",
      "\n",
      "Example Sentence Generation (Brown Bigrams):\n",
      "A man , and the first time .\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "#I made this generixc function which handles any ngram so that I don't have t write bi or trigram funtion seperately.\n",
    "def extract_ngrams(sentence, n=2):\n",
    "    tokens = sentence.split()\n",
    "    return list(zip(*[tokens[i:] for i in range(n)]))\n",
    "\n",
    "def build_ngram_model(corpus, n=2):\n",
    "    ngram_counts = Counter()\n",
    "    for sentence in corpus:\n",
    "        ngram_counts.update(extract_ngrams(sentence, n))\n",
    "    return ngram_counts\n",
    "\n",
    "\n",
    "bigrams_brown = build_ngram_model(brown_corpus, 2)\n",
    "trigrams_brown = build_ngram_model(brown_corpus, 3)\n",
    "bigrams_webtext = build_ngram_model(webtext_corpus, 2)\n",
    "trigrams_webtext = build_ngram_model(webtext_corpus, 3)\n",
    "bigrams_reuters = build_ngram_model(reuters_corpus, 2)\n",
    "trigrams_reuters = build_ngram_model(reuters_corpus, 3)\n",
    "\n",
    "\n",
    "def conditional_prob(w_given, context, ngram_counts, prefix_counts, vocab_size, laplace=True):\n",
    "    count_ngram = ngram_counts.get(context + (w_given,), 0)\n",
    "    count_prefix = prefix_counts.get(context, 0)\n",
    "    \n",
    "    if laplace:\n",
    "        return (count_ngram + 1) / (count_prefix + vocab_size)\n",
    "    return count_ngram / count_prefix if count_prefix != 0 else 0\n",
    "\n",
    "\n",
    "def calculate_perplexity(name, sentence, ngram_counts, prefix_counts, vocab_size, n=2):\n",
    "    \"\"\"Calculate perplexity for a given sentence without using logarithms.\"\"\"\n",
    "    tokens = sentence.split()\n",
    "    num_tokens = len(tokens)\n",
    "    prob_product = 1.0 \n",
    "    \n",
    "    for i in range(num_tokens):\n",
    "        context = tuple(tokens[max(0, i - n + 1):i])\n",
    "\n",
    "        prob = conditional_prob(tokens[i], context, ngram_counts, prefix_counts, vocab_size)\n",
    "        \n",
    "        # Avoiding multiplication by zero by adding a small smoothing factor\n",
    "        prob_product *= (prob + 1e-10)\n",
    "\n",
    "    # Compute perplexity directly using n-th root\n",
    "    perplexity = prob_product ** (-1 / num_tokens)\n",
    "    \n",
    "    return perplexity\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    brown_unigrams = Counter(token for sent in brown_corpus for token in sent.split())\n",
    "    webtext_unigrams = Counter(token for sent in webtext_corpus for token in sent.split())\n",
    "    \n",
    "    test_sentences = brown_corpus[10:15]    \n",
    "\n",
    "    bg_ppl = [calculate_perplexity(\"brownBG\", sent, bigrams_brown, brown_unigrams, \n",
    "                                  len(brown_unigrams), 2) for sent in test_sentences]\n",
    "    avg_bg_ppl = sum(bg_ppl)/len(bg_ppl)\n",
    "    \n",
    "\n",
    "    tg_ppl = [calculate_perplexity(\"brownTG\",sent, trigrams_brown, bigrams_brown,\n",
    "                                  len(brown_unigrams), 3) for sent in test_sentences]\n",
    "    avg_tg_ppl = sum(tg_ppl)/len(tg_ppl)\n",
    "    \n",
    "    print(\"\\nQuestion 1 Results:\")\n",
    "    print(f\"Brown Bigram Avg Perplexity: {avg_bg_ppl:.2f}\")\n",
    "    print(f\"Brown Trigram Avg Perplexity: {avg_tg_ppl:.2f}\")\n",
    "    print(\"Expected Trigram perplexity to be lower, but it might be higher due to data sparsity\")\n",
    "    \n",
    "\n",
    "    \n",
    "    reuters_unigrams = Counter(token for sent in brown_corpus for token in sent.split())    \n",
    "    test_sentences_reuters = reuters_corpus[30:55]    \n",
    "\n",
    "    bg_ppl_reuters = [calculate_perplexity(\"webtextBG\", sent, bigrams_webtext, webtext_unigrams, \n",
    "                                  len(webtext_unigrams), 2) for sent in test_sentences_reuters]\n",
    "    avg_bg_ppl_reuters = sum(bg_ppl_reuters)/len(bg_ppl_reuters)\n",
    "    \n",
    "\n",
    "    tg_ppl_reuters = [calculate_perplexity(\"webtextTG\",sent, trigrams_webtext, bigrams_webtext,\n",
    "                                  len(webtext_unigrams), 3) for sent in test_sentences_reuters]\n",
    "    avg_tg_ppl_reuters = sum(tg_ppl_reuters)/len(tg_ppl_reuters)\n",
    "    \n",
    "\n",
    "    print(\"\\nQuestion 2 Results:\")\n",
    "    print(f\"Webtext Bigram Avg PPL on Reuters: {avg_bg_ppl_reuters:.2f}\")\n",
    "    print(f\"Webtext Trigram Avg PPL on Reuters: {avg_tg_ppl_reuters:.2f}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    bg_ppl_reuters_b = [calculate_perplexity(\"brownBG\", sent, bigrams_brown, brown_unigrams, \n",
    "                                  len(brown_unigrams), 2) for sent in test_sentences_reuters]\n",
    "    avg_bg_ppl_reuters_b = sum(bg_ppl_reuters_b)/len(bg_ppl_reuters_b)\n",
    "    \n",
    "\n",
    "    tg_ppl_reuters_b = [calculate_perplexity(\"brownTG\",sent, trigrams_brown, bigrams_brown,\n",
    "                                  len(brown_unigrams), 3) for sent in test_sentences_reuters]\n",
    "    avg_tg_ppl_reuters_b = sum(tg_ppl_reuters_b)/len(tg_ppl_reuters_b)\n",
    "    \n",
    "\n",
    "    print(\"\\nQuestion 2 Results:\")\n",
    "    print(f\"Brown Bigram Avg PPL on Reuters: {avg_bg_ppl_reuters_b:.2f}\")\n",
    "    print(f\"Brown Trigram Avg PPL on Reuters: {avg_tg_ppl_reuters_b:.2f}\")\n",
    "\n",
    "\n",
    "    \n",
    "    print(\"\\nExample Sentence Generation (Brown Trigrams):\")\n",
    "    print(predict_sentence(\"justice\", trigrams_brown, bigrams_brown,\n",
    "                          len(brown_unigrams)))\n",
    "    \n",
    "    print(\"\\nExample Sentence Generation (Brown Bigrams):\")\n",
    "    print(predict_sentence(\"A man\", bigrams_brown, brown_unigrams,\n",
    "                          len(brown_unigrams)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20da4048-0886-4ccf-9357-43e247618dff",
   "metadata": {},
   "source": [
    "**Questions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1f63b6-00ef-41e5-b0d4-2f9c305c7aff",
   "metadata": {},
   "source": [
    "1. What do you expect the difference between the Brown bigram and trigram models to look like? Which model\n",
    "will provide you with more coherent text? How will the perplexity of each compare? You should test your\n",
    "predictor and perplexity function using the brown_bigrams and brown_trigrams to confirm your expectations. For perplexity, an average over 2-5 sentences from the Brown corpus should be fine, but make sure you\n",
    "use the same sentences both times. If something you did not expect occurs, explain what happened and why you\n",
    "believe it happened."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189cb4ad-ce5c-4c80-a58d-d463595fdca5",
   "metadata": {},
   "source": [
    "**Answer:** As per the theoratical study and the the concepts which I understood in class, trigrams should generate more relatable and readable sentences given a corpus. The trigram model should give us the more coherent text.\n",
    "I have tested my predictor fucntion and perplexity function using the brown_bigrams and brown_trigrams (5 random sentences taken from the corpus and kept same for both, trigrams and bigrams model) from the brown text corpus. I had expected that my model should have less perplexity for the trigrams model but the result said otherwise. I could see the bigram perplexity was lower than the trigram, which was unexpected.\n",
    "In a way I could make sense of it as, the corpus (5 lines) are too low for the trigrams model to given proper perplexity but having three set of words appear in 5 sentences has a low probability in this small samle space. As perplexity is inversely proportional to the Probability of the occuring words, the perplexity increased for trigrams."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e5bfed-7e92-4d74-9623-202ddcb4e5e7",
   "metadata": {},
   "source": [
    "2. When testing our bigram models on the Reuters data, do you think a model trained on Brown or Webtext will\n",
    "perform best? Pick any 25 sentences from the Reuters corpus and calculate the average perplexity using each\n",
    "of your bigram datasets. Compare the results of each and provide explanation as to why you believe that one\n",
    "performed better than the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc26a7ff-4321-4b68-9770-4c33312b9bd3",
   "metadata": {},
   "source": [
    "**Answer:** When testing bigram models trained on the Brown and Webtext corpora on Reuters sentences, the Webtext model performed better, yielding a lower perplexity score on average. This indicates that Webtext's structure and vocabulary align more closely with Reuters' modern financial news style, whereas the Brown corpus contains older, more formal language that does not generalize as well.\n",
    "Comparing the performance results:\n",
    "\n",
    "Webtext contains more modern terms, making it more suited to Reutersâ€™ language which in general led to overlapping vocabulary.\n",
    "\n",
    "Financial and news-related text structures align better with Webtext than Brown.\n",
    "\n",
    "The Brown corpus includes legal, literary, and outdated news styles, which do not suit well to Reuters' financial news domain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15882cb-0703-45bf-b76b-9227e1f06957",
   "metadata": {},
   "source": [
    "3. When predicting the next word in a sentence, what do you believe would happen if we increased the number of\n",
    "sentences in our training data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3eaf9b3-f920-4b2c-b085-e1181559fd35",
   "metadata": {},
   "source": [
    "**Answer:** Increasing the number of sentences in the training data would improve next-word prediction by providing better probability estimates and reducing perplexity of the trigram and bigram models both. With more data, the model learns a wider range of word combinations, making predictions more accurate and reducing the chances of encountering unknown sequences. It will also help capture rare words and domain-specific terms, leading to a more balanced and natural output. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6549a6e-e552-4081-a58a-8a2f95ecde2d",
   "metadata": {},
   "source": [
    "# References Used for this assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f4c1da-35d9-4ab0-a822-5242878c38e8",
   "metadata": {},
   "source": [
    "1. https://web.stanford.edu/~jurafsky/slp3/3.pdf\n",
    "2. Lectrue Slides\n",
    "3. https://training.continuumlabs.ai/data/datasets/what-is-perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37f08ff-8846-450e-aa46-e9c75615dd33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
